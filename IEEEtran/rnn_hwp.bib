@article{loading_problem_for_recurrent_neural_networks_Gori2005,
	title = {The loading problem for recursive neural networks },
	journal = {Neural Networks},
	volume = {18},
	number = {8},
	pages = {1064 - 1079},
	year = {2005},
	note = {Neural Networks and Kernel Methods for Structured Domains},
	issn = {0893-6080},
	doi = {http://dx.doi.org/10.1016/j.neunet.2005.07.006},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608005001668},
	author = {Marco Gori and Alessandro Sperdut},
	keywords = {Recursive neural networks},
	keywords = {Loading problem}",
	keywords = {Local minima},
	abstract = {The present work deals with one of the major and not yet completely understood topics of supervised connectionist models. Namely, it investigates the relationships between the difficulty of a given learning task and the chosen neural network architecture. These relationships have been investigated and nicely established for some interesting problems in the case of neural networks used for processing vectors and sequences, but only a few studies have dealt with loading problems involving graphical inputs. In this paper, we present sufficient conditions which guarantee the absence of local minima of the error function in the case of learning directed acyclic graphs with recursive neural networks. We introduce topological indices which can be directly calculated from the given training set and that allows us to design the neural architecture with local minima free error function. In particular, we conceive a reduction algorithm that involves both the information attached to the nodes and the topology, which enlarges significantly the class of the problems with unimodal error function previously proposed in the literature.},
}

@article{generalization_of_backpropagation_with_application_to_recurrent_gas_market_model_Werbos1988,
	title = "Generalization of backpropagation with application to a recurrent gas market model ",
	journal = "Neural Networks ",
	volume = "1",
	number = "4",
	pages = "339 - 356",
	year = "1988",
	note = "",
	issn = "0893-6080",
	doi = "http://dx.doi.org/10.1016/0893-6080(88)90007-X",
	url = "http://www.sciencedirect.com/science/article/pii/089360808890007X",
	author = "Paul J. Werbos",
	keywords = "Backpropagation",
	keywords = "Recurrent",
	keywords = "Continuous time",
	keywords = "Reinforcement learning",
	keywords = "Energy models",
	keywords = "Prediction",
	keywords = "Modelling",
	keywords = "Cerebral cortex ",
	abstract = "Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research. This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place. "
}

@article{focused_backpropagation_algorithm_for_temporal_pattern_recognition_Mozer89,
	added-at = {2008-09-16T23:39:07.000+0200},
	author = {Mozer, M. C.},
	biburl = {http://www.bibsonomy.org/bibtex/2fb845d599cc9885415f1701f1dcda7d8/brian.mingus},
	description = {CCNLab BibTeX},
	interhash = {6bbff0ef70e1a01d55bf2dccd17ff51f},
	intrahash = {fb845d599cc9885415f1701f1dcda7d8},
	journal = {Complex Systems},
	keywords = {CCP JRR,},
	pages = {349-381},
	timestamp = {2008-09-16T23:40:45.000+0200},
	title = {A focused backpropagation algorithm for temporal pattern recognition},
	volume = 3,
	year = 1989
}

@Inbook{rnn_are_local_minima_prone_Cuallar2006,
	author="Cuellar, M.P.
	and Delgado, M.
	and Pegalajar, M.C.",
	editor="Chen, Chin-Sheng
	and Filipe, Joaquim
	and Seruca, Isabel
	and Cordeiro, Jos{\'e}",
	chapter="AN APPLICATION OF NON-LINEAR PROGRAMMING TO TRAIN RECURRENT NEURAL NETWORKS IN TIME SERIES PREDICTION PROBLEMS",
	title="Enterprise Information Systems VII",
	year="2006",
	publisher="Springer Netherlands",
	address="Dordrecht",
	pages="95--102",
	isbn="978-1-4020-5347-4",
	doi="10.1007/978-1-4020-5347-4_11",
	url="http://dx.doi.org/10.1007/978-1-4020-5347-4-11"
}



